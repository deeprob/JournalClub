# Slide 1
In today's journal club, I will talk about a tool called DeepGRN used to characterize Transcription factor binding sites across cell types.

# Slide 2
I chose this paper in particular for three reasons. Firstly, it directly aligns to our research area. Secondly, it introduces an additional layer to the current state of the art. Deep learning model architecture used to characterize TF binding. Their model achieves comparable/superior performance over existing architecture - that's for us to decide. Finally, deep learning models are often criticized because they are not easily interpretable and here, the author's talk about how the addition of their layer leads to improved interpretability. 


Before directly jumping into their model and results, I would like to go over some biological and computational background. What I am going to talk about next might be extremely basic and most of you already know these in much greater details. But I thought it was essential to go over these before we can judge their model and results as a group.

# Slide 3
In the next two slides, I will discuss why characterizing TF binding sites is necessary. Our body is made up of cells that are functionally different in spite of having the same DNA and now we know that the difference in function can be attributed to varying expression of genes in these cells. Gene expression can be controlled at various stages starting chromatin accessibility to post translational modification of the protein product. But I think and I guess most of you will agree that the most important stage is transcription where the the gene is being transcribed into an mRNA and that's where the TF come into play

# Slide 4
TFs are proteins that play a central role in gene transcription. TFs bind to some specific location in the genome called TF binding sites or motifs which are actually short 6-12 bp DNA sequences and the binding of TF to these motifs determines whether a gene gets transcribed into mRNA consequently regulating the expression of that gene into a finished product. We can experimentally determine where a TF binds to the genome using ChipSeq or ChipExo but conducting that experiment for all cell types across all species is not practical. Hence we have computational models. 

# Slide 5
I have divided the computational models developed till date to characterize TF binding sites into two categories, conventional and deep learning. The primary difference between the two is that the conventional model requires handcrafted features such as numerically encoded kmers whereas the deep learning model can learn high level features from the DNA sequence itself. There are also some model training related differences such as the deep learning model is much easily scalable allowing us to take advantage of current computational advances such as gpu, tpu or parallel processing. Also most importantly, the deep learning models show superior performance. 

This paper has developed a deep learning model to predict TF binding and it will be the primary focus of this presentation. Before directly discussing their architecture, I would like to go over some terms and basic algorithms which form the core of the current state of the art models and the model used in this paper.

# Slide 6
So first of is the multilayer perceptron which I like to think of as linear regression with a non-linear activation.
The first operation of a MLP is nothing but a linear regression operation where we take a dot product of a weight vector with an input vector x, add it to a bias term and calculate the output. The non-linear part comes from passing the output of the first layer through an activation function, sigmoid, tanh or relu element wise and calculating the final output which can be bound/unbound in our case. MLP are great at summarizing results and are often found at the end or the final layers of a deep learning model. And they are also referred as Dense layers, fully connected layers or even feed forward networks. 

# Slide 7
Next I will talk about a very important neural network called a convolutional neural network which are very adept at finding patterns in the dataset. The main operation of a CNN is the convolution operation which is like a window or kernel or a filter of weights passing through an input to find patterns in that input. SO if we have a sequence as an input, then the convolutional filters will scan the entire sequence and the weights in the filters represent the numerically encoded sequence as a feature map as it passes through the entire sequence. Now here, we are interested in finding the motifs where the TF will bind and the filters should identify that motif and give it a score that's higher than the score given to another motif which may not be preferred by the TF. For example, in this sequence, if we assume that ATT is the preferred motif that the TF binds to, then if the filter assigns a value of let's say 0.8 to it feature map, then it must assign a lower value to another motif like AGA  which the TF does not prefer. 

# Slide 8
Another very important neural network is RNN which were originally developed to find context within sequences and we use it to find context with and between motifs. We know that a TF might prefer a number of motifs, but even among those motifs there are some differences in it's preference and the RNN should capture that. It must also capture any interactions between motifs for example if a TF only binds when there are two distinct motifs each at a certain specific number of bases away, then RNNs should be able to recognize that preference. They are usually represented like this where we have an input layer, a hidden layer and an output layer and if we remove this feedback between the hidden layers, an RNN is nothing but an MLP. However, with the feed back and unrolled looks like a bunch of MLPs. For a particular time step t, while calculating the hidden vector, instead of an MLP where only the input layer is multiplied by the weight matrix, here the hidden layer of the previous time steps are also multiplied with another weight matrix, the two output are then summed up and passed through an activation which provides the hidden vector of time step t. Since the hidden vector of the previous time step is being used to calculate the hidden state of the current time step, that's the network ensures that it captures the context between sequential inputs. Here I have shown that the output is calculated at every timestep which is called a many to many mapping but in our case we are only interested to know whether the TF binds or not, thus we only need to calculate one single output and that final hidden state is used to do that since it is assumed that the final state will capture the context of all previous hidden states. 

# Slide 9
So the current state of the art models are based on these three neural networks that I just discussed. So the first layer is usually a CNN that's supposed to capture any important motifs present in our input sequences and pass it on to the RNN that capture context within and between those motifs.  Finally, the output of the hidden state of the RNN is passed through an MLP that summarizes the input into a single dimensional output, whether the TF binds to that genomics location of not. 

# Slide 10
Most of these models are adopted from the NLP domain. In the NLP tasks, it was observed that while the RNN is perfectly capable of capturing contextual information when the sequence length is small, as the sequence length starts to grow, the final hidden state might forget what the network has seen in initial hidden states. Thus the network won't capture long term dependencies. Hence the idea is to use all hidden states in some way instead of them throwing away while calculating the final output. That's where attention layers come in to play. Instead of blindly using all the hidden states as is to calculate the final output, we use a MLP to score each hidden state on the basis of their importance and then we use the weighted hidden states, to calculate the final output. This has worked well in the NLP domain and in this paper the authors wanted to test whether the concept of attention layers can be adopted to the TF binding site problem as well. 

At this moment I would like to point out that what I just described is the most basic form of the 4 networks and over the years people have modified them in some way or another to get the most out of these models. For example the authors in this study not only used an attention layer that I just described but also used a multi headed attention layer. But I believe that with this knowledge we are perfectly capable of judging their modeling architecture and results.

# Slide 11
Here's a view of their model architecture. They are entering a sequence and it's reverse complement as an input. The input layer is followed by a CNN layer and an LSTM layer. LSTM is nothing but a modified RNN, it is also bidirectional, so instead of calculating the hidden states in one direction, it is also calculated in the reverse direction. You have a sequence you calculate the hidden states, flip the sequence calculate another set of hidden states use the final hidden states from the forward and backward pass to calculate the output. Following the LSTM is an attention layer and finally an MLP to summarize the input into an output. 

# Slide 12
Let's also go over their study design. 

# Slide 13
Okay, now results. I think this is their main results but they have focused more on their model's performance and comparisons with existing models that won the DREAM challenge. But their main purpose was to compare the network w and w/o attention layers and that's summarized in this very neglected figure. For example these dot's are actually the TF and their different cell types but there are no legend to distinguish them. First noticeable thing drop in performance. No comments from the authors about why there might be a drop and intuitively I can't think of one. Their first hypothesis was that attention layer can provide better performance and here certainly we can see at least two cases where it does but then again  how many of these 13 cases are really significant. 

# Slide 14
The author's next claim was that the attention layer can improve interpretability. So again a little bit of background, currently one of the most used methods to interpret deep learning models is Saliency maps. So the motivation behind developing saliency maps is this, If we consider a linear model, then the weights of the model will be proportional to the importance of the input vector. But deep learning models are highly non-linear, so there is no direct mapping between weights and features as in the linear case. However, we can estimate the non linear function as a linear function at a point I_0 through the first order Taylor series expansion of that function. Expanding that function like this shows that the weights of the non-linear function is the the derivative of the score wrt the input at a point I_0. That is more of a mathematical derivation but if we look at a real life example of image classification, so here we have images of lions, a dog and a crane and if our model that's trying to classify these images has actually learnt something, then it should focus on some very specific regions in the data that directly relate with the task. So here it focuses on these particular pixels which have the faces of these animals and that's how we know that the model is not learning noise, it's actually learning from these faces. Similarly in our TF binding prediction problem, the model should focus on the motifs that the TF prefers to bind with. This calculation is very easy to implement using any deep learning modules.

# Slide 15
The author's claim that attention weights can improve interpretability. So if we assume that sequential ordering in our model is preserved i.e. if the motif near the starting location, then the attention weights of the model corresponding to those location should be higher than the attention weights given to other locations. And they have compared the scores from saliency maps with the scores from attention weights and shown that attention weights are correlated to the actual binding location determined by chipseq than saliency scores. 

# Slide 16
Discussion!     
   

            
